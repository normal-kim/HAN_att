{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_weight = torch.nn.Parameter( torch.Tensor( 100, 100 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "org = copy.deepcopy( word_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_weight.data.data.normal_(0, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
    "from pykospacing import Spacing\n",
    "from konlpy.tag import Okt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_model = ReviewHAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlret`rieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 패키지 및 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install kss\n",
    "import kss\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models import Word2Vec\n",
    "#nltk.download('punkt')\n",
    "#import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(sentiment_data['review'][:500].apply(lambda x: len(kss.split_sentences(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "ind = random.randint(0, 10000)\n",
    "print(ind)\n",
    "temp_X = [ okt.morphs(corpus[ind][i]) for i in range(len(corpus[ind])) ]\n",
    "temp_X = [''.join(temp_X[i]) for i in range(len(temp_X))]\n",
    "temp_X = [spacing(temp_X[i] ) for i in range(len(temp_X))]\n",
    "temp_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 형태소 분석기로 stem 추출했더니 잘못 나오는 경우가 있다...\n",
    "- pykospacing 이 완벽한 것은 아님.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data = pd.read_excel( 'sentiment_dataset.xlsx')\n",
    "corpus = sentiment_data['review'].apply(lambda x: kss.split_sentences(x)).tolist()\n",
    "sentiment_data['polarity'] = sentiment_data['polarity'].replace('negative', 0).replace('positive', 1)\n",
    "res = process_NVA(corpus)\n",
    "all_sentences = list(chain(*res))\n",
    "w2v_model = Word2Vec(sentences = all_sentences, size = 100, window = 3, min_count = 30, workers = 4, sg = 0)\n",
    "df_in = do_preprocessing(res, sentiment_data, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAN_dataset(Dataset):\n",
    "    def __init__(self, review_df):\n",
    "        self.review = review_df.reset_index(drop = True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.review)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        label = self.review.loc[idx, 'polarity']\n",
    "        w2v = self.review.loc[idx, 'w2v']\n",
    "        num_words = self.review.loc[idx, 'num_words']\n",
    "        num_sent = self.review.loc[idx, 'num_sent']\n",
    "        \n",
    "        sample = {'w2v': w2v, \n",
    "                 'label': label, \n",
    "                 'num_sent' : num_sent, \n",
    "                 'num_words': num_words}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [],
   "source": [
    "def han_collate_fn(samples):\n",
    "    labels = [sample['label'] for sample in samples ]\n",
    "    w2v = [sample['w2v'] for sample in samples]\n",
    "    doc_lengths = [sample['num_sent'] for sample in samples]\n",
    "    sent_lengths = [sample['num_words'] for sample in samples]\n",
    "    \n",
    "    bsz = len(labels)\n",
    "    batch_max_doc_length = max(doc_lengths)\n",
    "    batch_max_sent_length = max( [max(sl) if sl else 0 for sl in sent_lengths])\n",
    "    docs_tensor = torch.zeros((bsz, batch_max_doc_length, batch_max_sent_length, 100), \n",
    "                             dtype = torch.float)\n",
    "    sent_lengths_tensor = torch.zeros((bsz, batch_max_doc_length)).long()\n",
    "    \n",
    "    for doc_idx, doc in enumerate(w2v):\n",
    "        doc_length = doc_lengths[doc_idx]\n",
    "        sent_lengths_tensor[doc_idx, :doc_length] = torch.Tensor(sent_lengths[doc_idx])\n",
    "        for sent_idx, sent in enumerate(doc):\n",
    "            sent_length = sent_lengths[doc_idx][sent_idx]\n",
    "            docs_tensor[doc_idx, sent_idx, :sent_length, :] = torch.FloatTensor(sent)\n",
    "            \n",
    "    return ( docs_tensor, torch.Tensor(labels), torch.Tensor(doc_lengths), sent_lengths_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HanDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, params_dict, shuffle = True):\n",
    "        self.n_samples = len(dataset)\n",
    "        self.init_kwargs = {\n",
    "            'dataset': dataset, \n",
    "            'batch_size' : params_dict['batch_size'],\n",
    "            'collate_fn' : han_collate_fn, \n",
    "            'shuffle': shuffle\n",
    "        }\n",
    "        super().__init__(**self.init_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from IPython.display import HTML\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sentence_to_color(words, scores, sent_score):\n",
    "    sentencemap = matplotlib.cm.get_cmap('Blues')\n",
    "    wordmap = matplotlib.cm.get_cmap('PuBu')\n",
    "#     result = '<p><span style=\"margin:30px; padding:5px; background-color: {}\">'\\\n",
    "#         .format(matplotlib.colors.rgb2hex(sentencemap(sent_score))[:3])\n",
    "    result = '<p><span style=\"margin:1px; padding:2px; background-color: {}\">'\\\n",
    "       .format(matplotlib.colors.rgb2hex(sentencemap(sent_score)[:3]))\n",
    "    template = '<span class = \"barcode\"; style =\"color: black; background-color: {}\">{}</span>'\n",
    "    for word, score in zip(words, scores):\n",
    "        color = matplotlib.colors.rgb2hex(wordmap(score)[:3])\n",
    "        result += template.format(color, '&nbsp' + word + '&nbsp')\n",
    "    result += '</span><p>'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_sample(df_test, idx):\n",
    "    orig_doc = df_test.loc[idx, 'tokenized']\n",
    "    doc, num_sents, num_words = df_test.loc[idx, 'w2v'], df_test.loc[idx, 'num_sent'], df_test.loc[idx, 'num_words']\n",
    "    ground_truth = df_test.loc[idx, 'polarity']\n",
    "    samples = [{'w2v': doc, \n",
    "             'label': ground_truth, \n",
    "             'num_sent' : num_sents, \n",
    "             'num_words': num_words}]\n",
    "    docs_tensor, labels, doc_lengths, sent_lengths = han_collate_fn(samples)\n",
    "    \n",
    "    x_s = (docs_tensor, doc_lengths, sent_lengths )\n",
    "    return orig_doc, x_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "han_encoder = ReviewHAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Attention Visualization</h2><p><span style=\"margin:1px; padding:2px; background-color: #08306b\"><span class = \"barcode\"; style =\"color: black; background-color: #f4edf6\">&nbsp입술&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #fff7fb\">&nbsp이&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f4eef6\">&nbsp편한&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f5eff6\">&nbsp발림&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #fff7fb\">&nbsp이네&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f4eef6\">&nbsp오&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f5eef6\">&nbsp각질&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f2ecf5\">&nbsp완전히&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f3edf5\">&nbsp눌러주진&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f3edf5\">&nbsp않지만&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f3edf5\">&nbsp입술&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f2ecf5\">&nbsp컨디션&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #fff7fb\">&nbsp이&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f4edf6\">&nbsp좋다면&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f4eef6\">&nbsp지속&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #fff7fb\">&nbsp도&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f3edf5\">&nbsp오래됩니다&nbsp</span></span><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = random.randint(0, len(df_test)-1)\n",
    "result = visualize_att(han_encoder, df_test, ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Attention Visualization</h2><p><span style=\"margin:1px; padding:2px; background-color: #afd1e7\"><span class = \"barcode\"; style =\"color: black; background-color: #d2d3e7\">&nbsp청순포텐&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #fff7fb\">&nbsp이&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #d1d2e6\">&nbsp아니고&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #fff7fb\">&nbsp..&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #ced0e6\">&nbsp아파&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #ced0e6\">&nbsp보여요&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #fff7fb\">&nbsp..&nbsp</span></span><p><p><span style=\"margin:1px; padding:2px; background-color: #add0e6\"><span class = \"barcode\"; style =\"color: black; background-color: #f0eaf4\">&nbsp원래&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f1ebf5\">&nbsp입술&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #fff7fb\">&nbsp이&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f3edf5\">&nbsp붉은&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f3edf5\">&nbsp편&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #fff7fb\">&nbsp인데&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #fff7fb\">&nbsp이&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f1ebf4\">&nbsp거&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f0eaf4\">&nbsp바르면&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f1ebf5\">&nbsp머&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #fff7fb\">&nbsp다른&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f2ecf5\">&nbsp지도&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f3edf5\">&nbsp모르게&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f5eff6\">&nbsp아무&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f5eff6\">&nbsp발색&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f5eef6\">&nbsp없어요&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #fff7fb\">&nbspㅋㅋㅋ&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #fff7fb\">&nbspㅋ&nbsp</span></span><p><p><span style=\"margin:1px; padding:2px; background-color: #a8cee4\"><span class = \"barcode\"; style =\"color: black; background-color: #b5c4df\">&nbsp그냥&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #fff7fb\">&nbsp..&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #b4c4df\">&nbsp그렇네&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #b3c3de\">&nbsp오&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #fff7fb\">&nbsp..&nbsp</span></span><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "[['품절', '대란', '워', '색상', '발색', '감촉', '좋아요'], ['저', '커피잔', '안', '묻어나는', '틴트', '찾고', '있는데', '이건', '묻어나요']]\n",
      "attention_sentence:  [[0.457771897315979, 0.542228102684021]]\n",
      "attention_word:  [[[0.14150747656822205, 0.15101945400238037, 0.14407497644424438, 0.14840368926525116, 0.1267567127943039, 0.15417300164699554, 0.13406464457511902, 0.0, 0.0], [0.11969892680644989, 0.10887161642313004, 0.10335748642683029, 0.11200614273548126, 0.11016161739826202, 0.10936056822538376, 0.11279682070016861, 0.11514297872781754, 0.10860376060009003]]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Attention Visualization</h2><p><span style=\"margin:1px; padding:2px; background-color: #7cb7da\"><span class = \"barcode\"; style =\"color: black; background-color: #e8e4f0\">&nbsp품절&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #e7e3f0\">&nbsp대란&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #e8e4f0\">&nbsp워&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #e7e3f0\">&nbsp색상&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #ece7f2\">&nbsp발색&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #e6e2ef\">&nbsp감촉&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #eae6f1\">&nbsp좋아요&nbsp</span></span><p><p><span style=\"margin:1px; padding:2px; background-color: #5da5d1\"><span class = \"barcode\"; style =\"color: black; background-color: #ede8f3\">&nbsp저&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #efe9f3\">&nbsp커피잔&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #f0eaf4\">&nbsp안&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #eee9f3\">&nbsp묻어나는&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #eee9f3\">&nbsp틴트&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #efe9f3\">&nbsp찾고&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #eee9f3\">&nbsp있는데&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #eee8f3\">&nbsp이건&nbsp</span><span class = \"barcode\"; style =\"color: black; background-color: #efe9f3\">&nbsp묻어나요&nbsp</span></span><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = random.randint(0, len(df_test) )\n",
    "print(ind)\n",
    "orig_doc, x_s = get_test_sample(df_test, ind)\n",
    "docs, doc_lengths, sent_lengths = x_s\n",
    "print(orig_doc)\n",
    "v, a_it, a_i = han_encoder(docs, doc_lengths, sent_lengths)\n",
    "print('attention_sentence: ', a_i.data.tolist())\n",
    "print('attention_word: ', a_it.data.tolist())\n",
    "\n",
    "words = orig_doc\n",
    "sent_score = a_i.tolist()[0]\n",
    "word_score = a_it.tolist()[0]\n",
    "result = \"<h2>Attention Visualization</h2>\"\n",
    "for sent, word_att, sent_att in zip(words, word_score, sent_score):\n",
    "    result += map_sentence_to_color( sent, word_att, sent_att)\n",
    "    \n",
    "\n",
    "display(HTML(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Attention Visualization</h2><p><span style = \"margin:4px; padding:3px; background-color: #00\"><span class = \"barcode\"; style = \"color: black; background-color: #fee9ca\">&nbsp호&nbsp</span><span class = \"barcode\"; style = \"color: black; background-color: #feeace\">&nbsp호&nbsp</span><span class = \"barcode\"; style = \"color: black; background-color: #feeace\">&nbsp예뻐요&nbsp</span><span class = \"barcode\"; style = \"color: black; background-color: #fee9cb\">&nbsp발라&nbsp</span><span class = \"barcode\"; style = \"color: black; background-color: #fee5c1\">&nbsp지속&nbsp</span><span class = \"barcode\"; style = \"color: black; background-color: #fee5c1\">&nbsp가는&nbsp</span><span class = \"barcode\"; style = \"color: black; background-color: #fee6c4\">&nbsp거&nbsp</span><span class = \"barcode\"; style = \"color: black; background-color: #fee9ca\">&nbsp같아요&nbsp</span></span><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example.html', 'w') as f:\n",
    "    f.write(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "han_encoder = ReviewHAN().to('cuda:0')\n",
    "x_s = get_x_s(sample_x_s)\n",
    "docs, doc_lengths, sent_lengths = x_s\n",
    "v, a_it, a_i = han_encoder(docs, doc_lengths, sent_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 14])"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_it.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-dcd8bb9c11d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mparams_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mparams_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdl1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHanDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHAN_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#han_dat = HAN_dataset(df_in)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "params_dict = {}\n",
    "params_dict['batch_size'] = 4\n",
    "df_train, df_test = train_test_split(df_in)\n",
    "dl1 = HanDataLoader(HAN_dataset(df_train), params_dict)\n",
    "#han_dat = HAN_dataset(df_in)\n",
    "sample_x_s = iter(dl1).__next__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_in' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5885e8e3321f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_in' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 100\n",
    "word_gru_h_dim = 100\n",
    "word_gru_n_layers = 2\n",
    "sent_gru_h_dim = 100\n",
    "sent_gru_n_layers = 2\n",
    "word_att_dim = 200\n",
    "sent_att_dim = 200\n",
    "dropval = 0.2\n",
    "dropgru_s = 0.2\n",
    "dropgru_w = 0.2\n",
    "\n",
    "sent_gru = nn.GRU( 2 * word_gru_h_dim, sent_gru_h_dim, \n",
    "                                num_layers = sent_gru_n_layers, batch_first = True,\n",
    "                                bidirectional = True, dropout = dropgru_s)\n",
    "sent_layer_norm = nn.LayerNorm( 2 * sent_gru_h_dim, elementwise_affine= True)\n",
    "sent_attention = nn.Linear(2 * sent_gru_h_dim, sent_att_dim)\n",
    "sentence_context_vector = nn.Linear(sent_att_dim, 1, bias = False)\n",
    "\n",
    "        # word\n",
    "word_gru = nn.GRU(embed_dim, word_gru_h_dim, num_layers = word_gru_n_layers, \n",
    "                              batch_first = True, bidirectional = True, dropout = dropgru_w)\n",
    "word_layer_norm = nn.LayerNorm( 2*word_gru_h_dim, elementwise_affine=True)\n",
    "word_attention = nn.Linear( 2 * word_gru_h_dim, word_att_dim)\n",
    "word_context_vector = nn.Linear(word_att_dim, 1, bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_x_s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e2edeb066426>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#x_s = get_x_s(sample_x_s)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#docs, doc_lengths, sent_lengths = x_s\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_x_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_x_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_x_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_x_s' is not defined"
     ]
    }
   ],
   "source": [
    "#x_s = get_x_s(sample_x_s)\n",
    "#docs, doc_lengths, sent_lengths = x_s\n",
    "docs, doc_lengths, sent_lengths = sample_x_s[0], sample_x_s[2], sample_x_s[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Packing\n",
    "## 1-1 reorder\n",
    "doc_lengths, doc_perm_idx = doc_lengths.sort(dim = 0, descending = True)\n",
    "docs = docs[doc_perm_idx]\n",
    "sent_lengths = sent_lengths[doc_perm_idx]\n",
    "\n",
    "## 1-2 packing\n",
    "packed_sents = pack_padded_sequence(docs, lengths=doc_lengths.tolist(), batch_first = True)\n",
    "packed_sent_lengths = pack_padded_sequence( sent_lengths, lengths= doc_lengths.tolist(), \n",
    "                                          batch_first=True)\n",
    "valid_bsz_sent = packed_sents.batch_sizes\n",
    "\n",
    "# 2. Word Attention\n",
    "## 2-1. packing input data\n",
    "sents, sent_lengths = packed_sents.data, packed_sent_lengths.data\n",
    "# reorder\n",
    "sent_lengths, sent_perm_idx = sent_lengths.sort(dim = 0, descending = True)\n",
    "sents = sents[sent_perm_idx]\n",
    "\n",
    "# embedding done already, do dropout\n",
    "#sents = self.Dropout(sents)\n",
    "packed_words = pack_padded_sequence( sents, lengths = sent_lengths.tolist(), batch_first=True)\n",
    "valid_bsz_word = packed_words.batch_sizes\n",
    "\n",
    "##2-2 NN\n",
    "# hidden layer\n",
    "h_it, _ = word_gru( packed_words )\n",
    "h_it_normed = word_layer_norm(h_it.data)\n",
    "h_it_pad, _ = pad_packed_sequence ( h_it, batch_first = True )\n",
    "# attention module\n",
    "u_it = torch.tanh( word_attention( h_it_normed.data ))\n",
    "u_it_cv = word_context_vector( u_it ).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention weights\n",
    "a_it_exp = torch.exp( u_it_cv - u_it_cv.max() )\n",
    "a_it_exp_pad, _ = pad_packed_sequence( PackedSequence( a_it_exp, valid_bsz_word), \n",
    "                                     batch_first= True)\n",
    "a_it = a_it_exp_pad / torch.sum( a_it_exp_pad, dim = 1, keepdim = True)\n",
    "# output\n",
    "s_i = (h_it_pad * a_it.unsqueeze(2)).sum(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-3 reorder\n",
    "_, sent_unperm_idx = sent_perm_idx.sort(dim = 0, descending = False)\n",
    "s_i = s_i[sent_unperm_idx] \n",
    "a_it = a_it[sent_unperm_idx] \n",
    "\n",
    "# 3. Sentence Attention\n",
    "sents, word_att_weights = s_i, a_it\n",
    "#sents = self.Dropout(sents)\n",
    "\n",
    "# 3-1 NN\n",
    "# hidden layer\n",
    "h_i, _ = sent_gru(PackedSequence(sents, valid_bsz_sent))\n",
    "h_i_normed = sent_layer_norm( h_i.data )\n",
    "h_i_pad, _ = pad_packed_sequence( h_i, batch_first = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention module\n",
    "u_i = torch.tanh( sent_attention( h_i_normed.data ))\n",
    "u_i_cv = sentence_context_vector(u_i).squeeze(1)\n",
    "# attention weights\n",
    "a_i_exp = torch.exp( u_i_cv - u_i_cv.max() )\n",
    "a_i_exp_pad, _ = pad_packed_sequence( PackedSequence(a_i_exp, valid_bsz_sent), \n",
    "                                    batch_first = True )\n",
    "sent_att_weights = a_i_exp_pad / torch.sum( a_i_exp_pad, dim = 1, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document vector\n",
    "v = ( h_i_pad * sent_att_weights.unsqueeze(2)).sum(dim = 1)\n",
    "\n",
    "# 3-2 reorder\n",
    "word_att_weights, _ = pad_packed_sequence( PackedSequence( word_att_weights, valid_bsz_sent), \n",
    "                                         batch_first = True)\n",
    "_, doc_unperm_idx = doc_perm_idx.sort(dim = 0, descending = False)\n",
    "\n",
    "# 4. Final Output\n",
    "v = v[doc_unperm_idx] \n",
    "a_it = word_att_weights[ doc_unperm_idx ] \n",
    "a_i = sent_att_weights[ doc_unperm_idx ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['document'].nunique(), train_data['label'].nunique()\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "# 형태소 분석기 OKT를 사용한 토큰화 작업 (다소 시간 소요)\n",
    "okt = Okt()\n",
    "tokenized_data = []\n",
    "for sentence in train_data['document'][:10000]:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    tokenized_data.append(temp_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences = tokenized_data, size = 100, window = 5, min_count = 5, workers = 4, sg = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dat = train_data.values[[220]][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokeni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt.morphs(sample_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = []\n",
    "for element in okt.morphs(sample_dat):\n",
    "    try:\n",
    "        w2v.append( model[element] )\n",
    "    except:\n",
    "        w2v.append( np.zeros(100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 220, 163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, cont in enumerate(train_data.values):\n",
    "    print(i, cont)\n",
    "#train_data.values[[10]][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
